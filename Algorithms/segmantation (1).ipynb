{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "segmantation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Is1g68wXM67d",
        "colab_type": "text"
      },
      "source": [
        "pip installing our datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbFf1Vba4FiO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7cac15bf-b481-4392-df13-76e2adaf36d8"
      },
      "source": [
        "%pip install morfessor"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: morfessor in /usr/local/lib/python3.6/dist-packages (2.0.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMeyjLrNNBZf",
        "colab_type": "text"
      },
      "source": [
        "Unloading and prepping datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lobULS1s0v9r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bbef005c-4b69-45ea-df92-9dcefed2f691"
      },
      "source": [
        "from nltk.corpus import words\n",
        "nltk.download('words')\n",
        "\n",
        "# using nltk word corpus as training data\n",
        "words = words.words()\n",
        "outfile = open(\"words\", \"w\")\n",
        "for word in words:\n",
        "    outfile.write(word+\"\\n\")\n",
        "\n",
        "outfile.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgzsC5uoNGhh",
        "colab_type": "text"
      },
      "source": [
        "Training our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zU1U1pyw08Wu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "737ef13d-7e77-403a-d24b-1919db4ef68e"
      },
      "source": [
        "import math\n",
        "import morfessor\n",
        "\n",
        "# function for adjusting the counts of each compound\n",
        "def log_func(x):\n",
        "    return int(round(math.log(x + 1, 2)))\n",
        "\n",
        "infile = \"words\"\n",
        "io = morfessor.MorfessorIO()\n",
        "train_data = list(io.read_corpus_file(infile))\n",
        "model = morfessor.BaselineModel()\n",
        "model.load_data(train_data, count_modifier=log_func)\n",
        "model.train_batch()\n",
        "io.write_binary_model_file(\"model.bin\", model)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100% (235892 of 235892) |################| Elapsed Time: 0:02:22 Time:  0:02:22\n",
            "100% (235892 of 235892) |################| Elapsed Time: 0:02:28 Time:  0:02:28\n",
            "100% (235892 of 235892) |################| Elapsed Time: 0:02:29 Time:  0:02:29\n",
            "100% (235892 of 235892) |################| Elapsed Time: 0:02:30 Time:  0:02:30\n",
            "100% (235892 of 235892) |################| Elapsed Time: 0:02:30 Time:  0:02:30\n",
            "100% (235892 of 235892) |################| Elapsed Time: 0:02:29 Time:  0:02:29\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwL78yZzNI3m",
        "colab_type": "text"
      },
      "source": [
        "Running a test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuD20ltW1CPo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "37d5cc55-64f0-46bc-cf83-e3844c3c1343"
      },
      "source": [
        "\n",
        "model_file = \"model.bin\"\n",
        "io = morfessor.MorfessorIO()\n",
        "model = io.read_binary_model_file(model_file)\n",
        "\n",
        "word = \"running\"\n",
        "# for segmenting new words we use the viterbi_segment(compound) method\n",
        "print(model.viterbi_segment(word)[0][0])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAalumd0NNoS",
        "colab_type": "text"
      },
      "source": [
        "Pickling it to use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrhgqqzmK5xl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a74ec6ed-3a83-4266-bc71-37effcbdfe8d"
      },
      "source": [
        "import pickle\n",
        "model_file = \"model.bin\"\n",
        "\n",
        "\n",
        "bruh_file_write = open(\"segment.txt\", 'wb')\n",
        "bruh_file_read = open(\"segment.txt\", 'rb')\n",
        "pickle.dump(model_file, bruh_file_write)\n",
        "bruh_file_write.close()\n",
        "lmao = pickle.load(bruh_file_read)\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model.bin\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}